{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import swifter\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import concurrent.futures\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [12, 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"./Data/YelpReviews.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_text'] = df['review_text'].str.strip('\"') # Remove the quotes from the review text\n",
    "\n",
    "df['text_character_length'] = df['review_text'].str.len() # Calculate the length of the review text\n",
    "\n",
    "df['text_word_count'] = df['review_text'].str.split().str.len() # Calculate the word count of the review text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\",\n",
    "                              model=model_name,\n",
    "                              device='mps')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens are the smallest units of text that the model can process. They can be words, parts of words, or punctuation marks. For example, the word \"tokenization\" might be split into \"token\" and \"##ization\". It's how AI processes texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text):\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "df['token_count'] = df['review_text'].apply(count_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    filename='sentiment_analysis.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "output_file = 'sentiment_results.csv'   \n",
    "\n",
    "def save_result(result, idx, output_file=output_file):\n",
    "    \"\"\"Safely save a single result to CSV\"\"\"\n",
    "    df = pd.DataFrame([result], index=[idx])\n",
    "    \n",
    "    if not os.path.exists(output_file):\n",
    "        df.to_csv(output_file)\n",
    "    else:\n",
    "        # Open in append mode without headers\n",
    "        df.to_csv(output_file, mode='a', header=False)\n",
    "\n",
    "def analyze_long_text(text, idx, chunk_size=512):\n",
    "        # Tokenize and chunk the text\n",
    "        tokens = sentiment_pipeline.tokenizer.tokenize(text)\n",
    "        chunks = [tokens[i:i + 512] for i in range(0, len(tokens), 512)]\n",
    "        results = []\n",
    "        \n",
    "        # Process each chunk\n",
    "        for chunk in chunks:\n",
    "            chunk_text = sentiment_pipeline.tokenizer.convert_tokens_to_string(chunk)\n",
    "            result = sentiment_pipeline(chunk_text)[0]\n",
    "            results.append(result)\n",
    "        \n",
    "        # Aggregate results\n",
    "        pos_scores = [r['score'] for r in results if r['label'] == 'positive']\n",
    "        neg_scores = [r['score'] for r in results if r['label'] == 'negative']\n",
    "        neu_scores = [r['score'] for r in results if r['label'] == 'neutral']\n",
    "        \n",
    "        pos_avg = sum(pos_scores) / len(pos_scores) if pos_scores else 0\n",
    "        neg_avg = sum(neg_scores) / len(neg_scores) if neg_scores else 0\n",
    "        neu_avg = sum(neu_scores) / len(neu_scores) if neu_scores else 0\n",
    "        \n",
    "        result = {}\n",
    "        # Determine sentiment\n",
    "        if pos_avg > neg_avg and pos_avg > neu_avg:\n",
    "            result = {\"label\": \"positive\", \"score\": pos_avg}\n",
    "        elif neg_avg > pos_avg and neg_avg > neu_avg:\n",
    "            result = {\"label\": \"negative\", \"score\": neg_avg}\n",
    "        else:\n",
    "            result = {\"label\": \"neutral\", \"score\": neu_avg}\n",
    "            \n",
    "        # Save result safely\n",
    "        save_result(result, idx)\n",
    "        logging.info(f\"Processed review {idx} with sentiment: {result['label']} (score: {result['score']:.4f})\")\n",
    "        return idx, result\n",
    "\n",
    "def process_reviews(df, num_workers=4, output_file=output_file):\n",
    "    # Load existing results if any\n",
    "    if os.path.exists(output_file):\n",
    "        existing_results = pd.read_csv(output_file, index_col=0)\n",
    "        processed_indices = set(existing_results.index)\n",
    "        logging.info(f\"Found {len(processed_indices)} existing results\")\n",
    "    else:\n",
    "        processed_indices = set()\n",
    "    \n",
    "    # Prepare unprocessed reviews\n",
    "    reviews_to_process = [\n",
    "        (text, idx) for idx, text in df['review_text'].items()\n",
    "        if idx not in processed_indices\n",
    "    ]\n",
    "    \n",
    "    if not reviews_to_process:\n",
    "        logging.info(\"All reviews already processed\")\n",
    "        return pd.read_csv(output_file, index_col=0)\n",
    "    \n",
    "    logging.info(f\"Processing {len(reviews_to_process)} reviews\")\n",
    "    \n",
    "    # Process reviews using ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = []\n",
    "        for text, idx in reviews_to_process:\n",
    "            futures.append(executor.submit(analyze_long_text, text, idx))\n",
    "        \n",
    "        # Show progress bar\n",
    "        for _ in tqdm(\n",
    "            concurrent.futures.as_completed(futures),\n",
    "            total=len(reviews_to_process),\n",
    "            desc=\"Processing reviews\"\n",
    "        ):\n",
    "            pass\n",
    "    \n",
    "    # Return final results\n",
    "    return pd.read_csv(output_file, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd43a0b6bb0e483687a25c44e04a4682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing reviews:   0%|          | 0/21761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_workers = mp.cpu_count() - 1  # Leave one CPU free\n",
    "\n",
    "# sentiment_results = process_reviews(df, num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.984932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.871294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.628548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.986272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.685562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21761</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.872460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21762</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.806394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21763</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.927083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21764</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.979736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21765</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.959155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21766 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment  sentiment_confidence\n",
       "0      positive              0.984932\n",
       "1      positive              0.871294\n",
       "2       neutral              0.628548\n",
       "3      positive              0.986272\n",
       "4       neutral              0.685562\n",
       "...         ...                   ...\n",
       "21761  positive              0.872460\n",
       "21762  positive              0.806394\n",
       "21763  positive              0.927083\n",
       "21764  positive              0.979736\n",
       "21765  positive              0.959155\n",
       "\n",
       "[21766 rows x 2 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_results = pd.read_csv('sentiment_results.csv', index_col=0)\n",
    "sentiment_results.sort_index(inplace=True)\n",
    "sentiment_results.rename(columns={'label': 'sentiment', 'score': 'sentiment_confidence'}, inplace=True)\n",
    "sentiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(sentiment_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "# ... existing code ...\n",
    "\n",
    "# Initialize emotion classifier\n",
    "emotion_pipeline = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"SamLowe/roberta-base-go_emotions\",\n",
    "    device='mps',  # Using your existing MPS setup\n",
    "    top_k=None\n",
    ")\n",
    "\n",
    "def process_emotions(texts, max_length=514):\n",
    "    return emotion_pipeline(\n",
    "        list(texts),  # Convert to list for batch processing\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        batch_size=32  # Add batching for efficiency\n",
    "    )\n",
    "\n",
    "# df.head(100)['review_text'].apply(emotion_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'admiration', 'score': 0.7070626020431519},\n",
       "  {'label': 'approval', 'score': 0.4086913764476776},\n",
       "  {'label': 'joy', 'score': 0.08597956597805023},\n",
       "  {'label': 'neutral', 'score': 0.07182684540748596},\n",
       "  {'label': 'pride', 'score': 0.024678487330675125},\n",
       "  {'label': 'relief', 'score': 0.0151624521240592},\n",
       "  {'label': 'optimism', 'score': 0.01454014889895916},\n",
       "  {'label': 'caring', 'score': 0.014052682556211948},\n",
       "  {'label': 'realization', 'score': 0.0136788971722126},\n",
       "  {'label': 'gratitude', 'score': 0.012014185078442097},\n",
       "  {'label': 'excitement', 'score': 0.011641857214272022},\n",
       "  {'label': 'annoyance', 'score': 0.006794773042201996},\n",
       "  {'label': 'love', 'score': 0.005275102332234383},\n",
       "  {'label': 'disapproval', 'score': 0.0043527353554964066},\n",
       "  {'label': 'amusement', 'score': 0.002929864451289177},\n",
       "  {'label': 'desire', 'score': 0.002471545012667775},\n",
       "  {'label': 'disappointment', 'score': 0.0020226691849529743},\n",
       "  {'label': 'anger', 'score': 0.0014279893366619945},\n",
       "  {'label': 'sadness', 'score': 0.0010429308749735355},\n",
       "  {'label': 'surprise', 'score': 0.0010070431744679809},\n",
       "  {'label': 'disgust', 'score': 0.0008440919918939471},\n",
       "  {'label': 'curiosity', 'score': 0.0007571291644126177},\n",
       "  {'label': 'confusion', 'score': 0.0006936745485290885},\n",
       "  {'label': 'grief', 'score': 0.0006778063252568245},\n",
       "  {'label': 'embarrassment', 'score': 0.0005479052779264748},\n",
       "  {'label': 'fear', 'score': 0.0005073889042250812},\n",
       "  {'label': 'remorse', 'score': 0.0004540592199191451},\n",
       "  {'label': 'nervousness', 'score': 0.0004179143288638443}]]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_pipeline(df.head(1)['review_text'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'admiration', 'score': 0.7070626616477966},\n",
       " {'label': 'admiration', 'score': 0.7006131410598755},\n",
       " {'label': 'admiration', 'score': 0.8200393319129944},\n",
       " {'label': 'admiration', 'score': 0.8702364563941956},\n",
       " {'label': 'joy', 'score': 0.41233178973197937}]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
